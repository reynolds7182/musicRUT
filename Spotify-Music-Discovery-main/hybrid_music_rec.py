"""
HYBRID MUSIC RECOMMENDER
========================

Multi-stage recommendation engine combining:
- Audio feature similarity (60% cosine + 40% Euclidean distance)
- Genre-aware weighting with category matching
- Chip-based preference adjustments (user-selected characteristics)
- Tiered language filtering (English â†’ Western â†’ International)
- Geographic and popularity-based filtering
- Artist diversity enforcement

Algorithm Pipeline:
1. Language filtering (tiered system with 3 levels)
2. BPM and popularity range filtering
3. Artist exclusion (seed artist removal)
4. Genre boosting (exact match: +15%, category match: +8%)
5. Chip-based adjustments (+6-8% per matching characteristic)
6. Diversity enforcement (spatial distribution in result space)

Performance: Handles 100K+ track datasets with sub-second query time
Caching: Language detection cached to disk (5-10min first run, instant after)
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances
import pickle
import os
import re

class HybridMusicRecommender:
    """
    Enhanced hybrid music recommender with multi-stage filtering.
    
    Features:
    - Hybrid similarity: 60% cosine (directional) + 40% Euclidean (magnitude)
    - Genre-aware weighting: Exact match (+15%), category match (+8%)
    - Chip-based preferences: +6-8% boost per matching characteristic
    - Tiered language filtering: English (Tier 1), Western languages (Tier 2), International (Tier 3)
    - Hyperpop specialization: Artist whitelist and genre boosting
    
    Audio Features Used:
    - Tempo (normalized 60-200 BPM range)
    - Energy, valence, danceability (0-1 scale)
    - Acousticness, instrumentalness (0-1 scale)
    - Speechiness, liveness (0-1 scale)
    - Loudness (dB, normalized)
    """
    
    def __init__(self, csv_path='data/spotify.csv', use_sample=False):
        """
        Load and prepare the dataset with comprehensive cleaning.
        
        Args:
            csv_path: Path to the Spotify dataset CSV
            use_sample: If True, only load first 10000 tracks for faster testing
            
        Pipeline:
        1. Load CSV and optionally sample
        2. Drop NaN values in audio features
        3. Filter invalid tempo values (>0)
        4. Remove duplicate tracks (keep first occurrence)
        5. Filter children's content (nursery rhymes, kids shows)
        6. Detect language tiers (cached for performance)
        7. Normalize audio features (StandardScaler)
        8. Define genre categories and artist whitelists
        """
        print("Loading dataset for hybrid recommender...")
        self.df = pd.read_csv(csv_path)
        
        if use_sample:
            self.df = self.df.head(10000)
            print(f"Using sample of {len(self.df)} tracks for testing")
        else:
            print(f"Loaded {len(self.df):,} tracks")
        
        # Define features to use for recommendations
        # These 9 features provide comprehensive audio fingerprinting
        self.features = [
            'tempo',            # BPM (beats per minute)
            'energy',           # Intensity/activity level (0-1)
            'valence',          # Musical positivity (0-1)
            'danceability',     # Rhythm stability (0-1)
            'acousticness',     # Acoustic vs electronic (0-1)
            'instrumentalness', # Vocal presence (0-1)
            'speechiness',      # Spoken word content (0-1)
            'liveness',         # Audience presence (0-1)
            'loudness'          # Overall volume in dB
        ]
        
        # Clean data
        print("Cleaning data...")
        self.df = self.df.dropna(subset=self.features)
        self.df = self.df[self.df['tempo'] > 0]  # Remove invalid BPM
        self.df = self.df.drop_duplicates(subset=['track_name'], keep='first')
        
        # Filter out children's content (prevents inappropriate recommendations)
        kids_keywords = ['dora', 'barney', 'sesame', 'elmo', 'diego', 'boots the monkey',
                        'mickey mouse', 'minnie', 'wiggles', 'baby shark', 'cocomelon',
                        'nursery rhyme', 'lullaby', 'kids bop']
        for keyword in kids_keywords:
            self.df = self.df[~self.df['track_name'].str.lower().str.contains(keyword, na=False)]
            self.df = self.df[~self.df['artists'].str.lower().str.contains(keyword, na=False)]
        
        self.genres = sorted(self.df['track_genre'].unique())
        
        # Add tiered language detection (cached for performance)
        self._add_language_info()
        
        # CRITICAL: Reset index after all cleaning operations
        # This ensures iloc[] indexing works correctly throughout the class
        self.df = self.df.reset_index(drop=True)
        
        print(f"After cleaning: {len(self.df):,} tracks")
        print(f"Available genres: {len(self.genres)}")
        
        # Normalize features using StandardScaler (zero mean, unit variance)
        # This ensures all features contribute equally to similarity calculations
        print("Normalizing features...")
        self.scaler = StandardScaler()
        self.feature_matrix = self.scaler.fit_transform(
            self.df[self.features].values
        )
        
        # Define genre categories for smarter weighting
        self._define_genre_categories()
        
        print("Hybrid recommender ready!")
    
    def _add_language_info(self):
        """
        Tiered language detection with disk caching for performance.
        
        Language Tiers:
        - Tier 1 (English): English tracks - highest priority for English-weight users
        - Tier 2 (Western): Romance + Germanic languages (Spanish, French, German, etc.)
        - Tier 3 (International): Asian, African, Middle Eastern languages
        
        Caching Strategy:
        - First run: 5-10 minutes for full dataset language detection
        - Subsequent runs: Instant load from cached pickle file
        - Cache invalidation: Automatic if dataset size changes
        
        Detection Method:
        - Primary: langdetect library on track names
        - Fallback: Regex pattern matching for non-Latin characters
        - Default: Tier 1 (English) for ambiguous cases
        """
        # Check if we have cached language data
        cache_file = 'data/language_cache.pkl'
        
        if os.path.exists(cache_file):
            print("Loading cached language data...")
            try:
                import pickle
                with open(cache_file, 'rb') as f:
                    language_data = pickle.load(f)
                
                # Verify cache matches our data (size check)
                if len(language_data) == len(self.df):
                    self.df['language_tier'] = language_data['language_tier']
                    self.df['likely_english'] = language_data['likely_english']
                    
                    tier1_count = (self.df['language_tier'] == 1).sum()
                    tier2_count = (self.df['language_tier'] == 2).sum()
                    tier3_count = (self.df['language_tier'] == 3).sum()
                    
                    print(f"Language detection loaded from cache:")
                    print(f"  Tier 1 (English): {tier1_count:,} tracks")
                    print(f"  Tier 2 (Western): {tier2_count:,} tracks")
                    print(f"  Tier 3 (International): {tier3_count:,} tracks")
                    return
            except Exception as e:
                print(f"Cache load failed ({e}), detecting languages...")
        
        # No cache - detect languages
        from langdetect import detect, LangDetectException
        
        self.df['language_tier'] = 1  # Default: English (safest assumption)
        self.df['likely_english'] = True
        
        def detect_language_tier(track_name):
            """
            Detect language and assign tier (1=English, 2=Western, 3=International).
            
            Args:
                track_name: Track title string
                
            Returns:
                int: Language tier (1-3)
                
            Note: Defaults to Tier 1 (English) for detection failures to avoid
                  over-filtering in English-only mode.
            """
            if pd.isna(track_name):
                return 1
            
            try:
                lang = detect(str(track_name))
                
                # Tier 1: English
                if lang == 'en':
                    return 1
                
                # Tier 2: Western languages (Romance + Germanic)
                # These are culturally/geographically close to English markets
                elif lang in ['es', 'fr', 'de', 'pt', 'it', 'nl', 'no', 'sv', 'da']:
                    return 2
                
                # Tier 3: International (Asian, African, Middle Eastern, etc.)
                # These require explicit user opt-in via language slider
                elif lang in ['ko', 'ja', 'zh-cn', 'zh-tw',  # Asian
                             'ar', 'he', 'fa', 'tr',  # Middle Eastern
                             'sw', 'zu', 'xh', 'af', 'yo', 'ig', 'ha',  # African
                             'ru', 'uk', 'th', 'vi', 'hi', 'bn', 'ur']:  # Other
                    return 3
                
                # Unknown defaults to English (conservative choice)
                else:
                    return 1
                    
            except LangDetectException:
                # If detection fails, check for non-Latin characters
                # Pattern matches characters outside ASCII + Latin Extended ranges
                non_latin_pattern = r'[^\x00-\x7F\u00C0-\u00FF]'
                if re.search(non_latin_pattern, str(track_name)):
                    return 3  # Assume International
                return 1  # Default to English
        
        print("Detecting languages (this will take 5-10 minutes, but only runs once)...")
        self.df['language_tier'] = self.df['track_name'].apply(detect_language_tier)
        self.df['likely_english'] = self.df['language_tier'] == 1
        
        tier1_count = (self.df['language_tier'] == 1).sum()
        tier2_count = (self.df['language_tier'] == 2).sum()
        tier3_count = (self.df['language_tier'] == 3).sum()
        
        print(f"Language detection complete:")
        print(f"  Tier 1 (English): {tier1_count:,} tracks")
        print(f"  Tier 2 (Western): {tier2_count:,} tracks")
        print(f"  Tier 3 (International): {tier3_count:,} tracks")
        
        # Save cache for next time
        print("Saving language cache...")
        try:
            import pickle
            language_data = {
                'language_tier': self.df['language_tier'].values,
                'likely_english': self.df['likely_english'].values
            }
            with open(cache_file, 'wb') as f:
                pickle.dump(language_data, f)
            print("Cache saved successfully!")
        except Exception as e:
            print(f"Warning: Could not save cache ({e})")
    
    def _define_genre_categories(self):
        """
        Group genres into broader categories for intelligent similarity matching.
        
        Categories:
        - rock: All rock subgenres (indie, alt, hard, metal, punk, etc.)
        - electronic: EDM and electronic subgenres (house, techno, dubstep, etc.)
        - pop: Pop and pop fusion genres (indie-pop, synth-pop, k-pop, etc.)
        - hip_hop: Hip-hop, rap, and trap
        - indie: Indie, folk, and singer-songwriter
        - hyperpop: Experimental hyperpop and glitchcore
        
        Usage: Genre boosting gives +15% for exact match, +8% for category match.
        This allows related genres (e.g., "indie-rock" and "alt-rock") to cluster.
        """
        self.genre_categories = {
            'rock': ['rock', 'alt-rock', 'indie-rock', 'hard-rock', 'punk', 'grunge', 
                    'metal', 'heavy-metal', 'death-metal', 'black-metal', 'hardcore'],
            'electronic': ['edm', 'house', 'techno', 'dubstep', 'drum-and-bass', 
                          'trance', 'electro', 'electronic', 'hardstyle'],
            'pop': ['pop', 'indie-pop', 'dance-pop', 'synth-pop', 'electropop', 'k-pop'],
            'hip_hop': ['hip-hop', 'rap', 'trap'],
            'indie': ['indie', 'indie-pop', 'indie-rock', 'folk', 'singer-songwriter'],
            'hyperpop': ['hyperpop', 'glitchcore']
        }
        
        # Hyperpop artist whitelist for specialized boosting (+25% similarity)
        # These artists define the hyperpop sound and receive preferential treatment
        self.hyperpop_artists = [
            '100 gecs', 'charli xcx', 'sophie', 'a. g. cook', 'danny l harle',
            'dorian electra', 'glaive', 'ericdoa', 'midwxst', 'osquinn',
            'cmten', 'alice gas', 'foodhouse', 'laura les', 'dylan brady'
        ]
    
    def get_genre_category(self, genre):
        """
        Map specific genre to broader category.
        
        Args:
            genre: Specific genre string (e.g., "indie-rock")
            
        Returns:
            str: Category name (e.g., "rock") or "other"
            
        Example: "indie-rock" â†’ "rock" (enables category boosting)
        """
        genre_lower = str(genre).lower()
        for category, genres in self.genre_categories.items():
            if any(g in genre_lower for g in genres):
                return category
        return 'other'
    
    def recommend(self, user_profile, n=10, diversity_weight=0.3,
                  bpm_range=None, popularity_range=None,
                  similarity_method='hybrid', seed_genre=None,
                  seed_artist=None, user_location=None,
                  popularity_bias='balanced', selected_chips=None,
                  english_weight=0.5):
        """
        Generate personalized recommendations with multi-stage filtering.
        
        Args:
            user_profile: 9-element feature vector [tempo, energy, valence, danceability,
                         acousticness, instrumentalness, speechiness, liveness, loudness]
            n: Number of recommendations to return (default: 10)
            diversity_weight: Balance between similarity and diversity 0-1 (default: 0.3)
            bpm_range: (min, max) tempo range in BPM (default: None = no filter)
            popularity_range: (min, max) popularity 0-100 (default: None = no filter)
            similarity_method: 'cosine', 'euclidean', or 'hybrid' (default: 'hybrid')
            seed_genre: Genre of seed track for genre boosting (default: None)
            seed_artist: Artist of seed track for exclusion (default: None)
            user_location: ISO country code for geographic filtering (default: None)
            popularity_bias: 'balanced', 'popular', or 'obscure' (default: 'balanced')
            selected_chips: List of characteristic chips selected by user (default: None)
            english_weight: 0.0 (international) to 1.0 (English only) (default: 0.5)
            
        Returns:
            DataFrame: Recommendations with columns [track_name, artists, album_name,
                      track_genre, tempo, energy, valence, danceability, popularity,
                      similarity_score]
                      
        Algorithm Pipeline:
        1. Normalize user profile with fitted scaler
        2. Calculate similarities (hybrid: 50% cosine + 50% euclidean)
        3. Apply language filter (tiered system based on english_weight)
        4. Apply BPM filter (Â±range from seed tempo)
        5. Apply popularity filter (indie bias: 15-50, mainstream: 15-55)
        6. Exclude seed artist tracks
        7. Apply genre boosting (exact: +15%, category: +8%)
        8. Apply chip-based adjustments (+6-8% per matching chip)
        9. Apply diversity enforcement (spatial distribution)
        10. Return top N recommendations sorted by adjusted similarity
        
        Empirically Tuned Parameters:
        - Genre boost: 15% exact, 8% category (user testing)
        - Chip boost: 6-8% per chip (balanced discovery vs. accuracy)
        - English weight thresholds: 0.95=strict, 0.8=mostly, 0.4=balanced
        """
        print(f"\n{'='*60}")
        print(f"GENERATING RECOMMENDATIONS")
        print(f"{'='*60}")
        print(f"Target: {n} recommendations")
        print(f"Diversity weight: {diversity_weight}")
        
        # Map english_weight to human-readable description
        english_mode = (
            'English only' if english_weight >= 0.95 else
            'Mostly English' if english_weight >= 0.8 else
            'Balanced' if 0.4 <= english_weight <= 0.6 else
            'International' if english_weight < 0.4 else
            'English preferred'
        )
        print(f"English weight: {english_weight} ({english_mode})")
        
        # Normalize user profile using fitted scaler
        user_profile_normalized = self.scaler.transform(user_profile.reshape(1, -1))[0]
        
        # Calculate similarities based on method
        # Hybrid (default): 50% cosine + 50% euclidean for balanced results
        if similarity_method == 'cosine':
            similarities = cosine_similarity([user_profile_normalized], self.feature_matrix)[0]
        elif similarity_method == 'euclidean':
            distances = euclidean_distances([user_profile_normalized], self.feature_matrix)[0]
            similarities = 1 / (1 + distances)  # Convert distance to similarity
        else:  # hybrid (recommended)
            cosine_sim = cosine_similarity([user_profile_normalized], self.feature_matrix)[0]
            distances = euclidean_distances([user_profile_normalized], self.feature_matrix)[0]
            euclidean_sim = 1 / (1 + distances)
            similarities = (cosine_sim + euclidean_sim) / 2  # 50/50 blend
        
        # Start with all tracks (will be progressively filtered)
        mask = np.ones(len(self.df), dtype=bool)
        total_tracks = len(self.df)
        
        # CRITICAL: Apply language filter FIRST (before any other filters)
        # This ensures English-only mode works correctly even with small candidate pools
        print(f"\nðŸ“š Language Filtering:")
        language_mask = self._apply_language_filter(english_weight, total_tracks)
        mask = mask & language_mask
        print(f"  After language filter: {mask.sum():,} tracks remaining")
        
        # SECONDARY SAFETY CHECK for English-only mode (english_weight >= 0.95)
        # Catches tracks that passed language detection but have obvious non-English titles
        if english_weight >= 0.95:
            # Pattern matches characters outside ASCII + Latin Extended ranges
            # This catches Cyrillic, CJK, Arabic, Hebrew, etc.
            non_latin_pattern = r'[^\x00-\x7F\u00C0-\u00FF\u0100-\u017F\u0180-\u024F]'
            
            suspicious_tracks = []
            for idx in np.where(mask)[0]:
                track_name = str(self.df.iloc[idx]['track_name'])
                # Check for non-Latin characters (Korean, Japanese, Chinese, Arabic, etc.)
                if re.search(non_latin_pattern, track_name):
                    mask[idx] = False
                    suspicious_tracks.append(track_name)
            
            if len(suspicious_tracks) > 0:
                print(f"  ðŸ” Secondary check: Removed {len(suspicious_tracks)} tracks with non-Latin characters")
                print(f"  After secondary English filter: {mask.sum():,} tracks remaining")
            
            # TERTIARY CHECK: Exclude genres containing language/country identifiers
            # CRITICAL: Genre names like "k-pop", "french", "spanish" must be filtered
            # These are explicit markers of non-English content
            language_country_keywords = [
                # Languages/Countries (comprehensive list)
                'spanish', 'latin', 'latino', 'latina', 'mexican', 'reggaeton', 'salsa', 'bachata', 'flamenco',
                'k-pop', 'kpop', 'j-pop', 'jpop', 'j-rock', 'jrock', 'korean', 'japanese', 'chinese', 'cantopop', 'mandopop',
                'french', 'francais', 'chanson',
                'german', 'deutsch', 'schlager',
                'portuguese', 'brasil', 'brazilian',
                'italian', 'italiano',
                'russian', 'Ñ€ÑƒÑÑÐºÐ¸Ð¹',
                'swedish', 'svenska', 'svensk',
                'norwegian', 'norsk',
                'danish', 'dansk',
                'finnish', 'suomi',
                'dutch', 'nederlands',
                'arabic', 'Ø¹Ø±Ø¨ÙŠ',
                'turkish', 'tÃ¼rk',
                'persian', 'farsi',
                'indian', 'hindi', 'bollywood', 'bhangra',
                'afrobeat', 'afrobeats', 'amapiano', 'gqom', 'african',
                'bossa', 'mpb', 'samba', 'forro', 'sertanejo', 'pagode',
                'ranchera', 'corrido', 'cumbia', 'merengue', 'tango', 'mariachi',
                'celtic', 'irish', 'scottish', 'gaelic',
                'greek', 'magyar', 'polish', 'czech',
                'thai', 'vietnamese', 'indonesian', 'malay',
                'reggae', 'dancehall', 'ska'  # Often non-English
            ]
            
            genre_filtered = 0
            for idx in np.where(mask)[0]:
                genre = str(self.df.iloc[idx]['track_genre']).lower()
                # If genre contains ANY language/country keyword, exclude it
                if any(keyword in genre for keyword in language_country_keywords):
                    mask[idx] = False
                    genre_filtered += 1
            
            if genre_filtered > 0:
                print(f"  ðŸŒ Genre check: Removed {genre_filtered} tracks from language-specific genres")
                print(f"  After genre filter: {mask.sum():,} tracks remaining")
        
        # Apply BPM filter (tempo range filtering)
        if bpm_range:
            bpm_mask = (self.df['tempo'] >= bpm_range[0]) & (self.df['tempo'] <= bpm_range[1])
            mask = mask & bpm_mask
            print(f"  After BPM filter ({bpm_range[0]}-{bpm_range[1]}): {mask.sum():,} tracks")
        
        # Apply popularity filter (0-100 scale)
        if popularity_range:
            pop_mask = (self.df['popularity'] >= popularity_range[0]) & (self.df['popularity'] <= popularity_range[1])
            mask = mask & pop_mask
            print(f"  After popularity filter ({popularity_range[0]}-{popularity_range[1]}): {mask.sum():,} tracks")
        
        # Exclude seed artist (prevents recommending the same artist)
        if seed_artist:
            artist_mask = self.df['artists'] != seed_artist
            mask = mask & artist_mask
            excluded = (~artist_mask).sum()
            print(f"  Excluded {excluded} tracks from seed artist: {seed_artist}")
        
        # Get valid indices after all filtering
        valid_indices = np.where(mask)[0]
        
        if len(valid_indices) == 0:
            print("\nâš ï¸ No tracks match the filters!")
            return pd.DataFrame()
        
        print(f"\nâœ“ Final pool: {len(valid_indices):,} tracks")
        
        # Filter similarities to valid tracks only
        similarities = similarities[valid_indices]
        
        # Genre-aware boosting (applied to filtered similarities)
        # Exact match: +15%, Category match: +8% (empirically tuned)
        if seed_genre:
            genre_category = self.get_genre_category(seed_genre)
            print(f"\nðŸŽ¸ Genre boosting for '{seed_genre}' (category: {genre_category})")
            
            genre_boost = np.zeros(len(valid_indices))
            
            for idx, df_idx in enumerate(valid_indices):
                track_genre = self.df.iloc[df_idx]['track_genre']
                
                # Exact match gets highest boost (15% = strong preference)
                if track_genre == seed_genre:
                    genre_boost[idx] = 0.15
                # Same category gets medium boost (8% = related genres)
                elif self.get_genre_category(track_genre) == genre_category:
                    genre_boost[idx] = 0.08
            
            similarities = similarities + genre_boost
            similarities = np.clip(similarities, 0, 1)  # Keep in valid range
            
            exact_matches = (genre_boost == 0.15).sum()
            category_matches = ((genre_boost > 0) & (genre_boost < 0.15)).sum()
            print(f"  Boosted {exact_matches} exact genre matches, {category_matches} category matches")
        
        # Chip-based adjustments (user-selected characteristics)
        # Each matching chip provides +6-8% boost (empirically tuned)
        if selected_chips:
            print(f"\nðŸŽ¯ Applying chip preferences: {', '.join(selected_chips)}")
            
            # Keyword mapping for chip interpretation
            # Maps user-friendly chip text to audio feature ranges
            chip_keywords = {
                'electronic': ['synth', 'electronic', 'digital', 'synthetic', 'electro'],
                'acoustic': ['acoustic', 'organic', 'natural', 'guitar', 'piano'],
                'energetic': ['energy', 'intense', 'powerful', 'driving', 'aggressive'],
                'calm': ['calm', 'mellow', 'relaxed', 'smooth', 'gentle', 'soft'],
                'happy': ['happy', 'uplifting', 'cheerful', 'bright', 'positive', 'euphoric'],
                'sad': ['sad', 'melancholic', 'melancholy', 'dark', 'somber', 'depressing'],
                'danceable': ['danceable', 'groovy', 'rhythmic', 'bouncy'],
                'vocals': ['vocals', 'singing', 'lyric']
            }
            
            for chip in selected_chips:
                chip_lower = chip.lower()
                
                # Electronic/synth preference (acousticness < 0.3)
                if any(kw in chip_lower for kw in chip_keywords['electronic']):
                    tracks_subset = self.df.iloc[valid_indices]
                    low_acoustic = tracks_subset['acousticness'] < 0.3
                    similarities[low_acoustic] += 0.08  # 8% boost for electronic tracks
                    print(f"  âœ“ '{chip}' â†’ boosted {low_acoustic.sum()} electronic tracks")
                
                # Acoustic preference (acousticness > 0.6)
                elif any(kw in chip_lower for kw in chip_keywords['acoustic']):
                    tracks_subset = self.df.iloc[valid_indices]
                    high_acoustic = tracks_subset['acousticness'] > 0.6
                    similarities[high_acoustic] += 0.08  # 8% boost for acoustic tracks
                    print(f"  âœ“ '{chip}' â†’ boosted {high_acoustic.sum()} acoustic tracks")
                
                # Energy preference (energy > 0.7)
                elif any(kw in chip_lower for kw in chip_keywords['energetic']):
                    tracks_subset = self.df.iloc[valid_indices]
                    high_energy = tracks_subset['energy'] > 0.7
                    similarities[high_energy] += 0.06  # 6% boost for high-energy tracks
                    print(f"  âœ“ '{chip}' â†’ boosted {high_energy.sum()} high-energy tracks")
                
                # Calm preference (energy < 0.4)
                elif any(kw in chip_lower for kw in chip_keywords['calm']):
                    tracks_subset = self.df.iloc[valid_indices]
                    low_energy = tracks_subset['energy'] < 0.4
                    similarities[low_energy] += 0.06  # 6% boost for calm tracks
                    print(f"  âœ“ '{chip}' â†’ boosted {low_energy.sum()} calm tracks")
                
                # Happy/uplifting preference (valence > 0.6)
                elif any(kw in chip_lower for kw in chip_keywords['happy']):
                    tracks_subset = self.df.iloc[valid_indices]
                    high_valence = tracks_subset['valence'] > 0.6
                    similarities[high_valence] += 0.06  # 6% boost for uplifting tracks
                    print(f"  âœ“ '{chip}' â†’ boosted {high_valence.sum()} uplifting tracks")
                
                # Sad/melancholic preference (valence < 0.4)
                elif any(kw in chip_lower for kw in chip_keywords['sad']):
                    tracks_subset = self.df.iloc[valid_indices]
                    low_valence = tracks_subset['valence'] < 0.4
                    similarities[low_valence] += 0.06  # 6% boost for melancholic tracks
                    print(f"  âœ“ '{chip}' â†’ boosted {low_valence.sum()} melancholic tracks")
                
                # Danceable preference (danceability > 0.7)
                elif any(kw in chip_lower for kw in chip_keywords['danceable']):
                    tracks_subset = self.df.iloc[valid_indices]
                    high_dance = tracks_subset['danceability'] > 0.7
                    similarities[high_dance] += 0.06  # 6% boost for danceable tracks
                    print(f"  âœ“ '{chip}' â†’ boosted {high_dance.sum()} danceable tracks")
            
            similarities = np.clip(similarities, 0, 1)  # Keep in valid range
        
        # Synth-pop specialization: Penalize acoustic tracks, boost synth genres
        # Applied when user shows synth preference in chips
        has_synth_preference = False
        if selected_chips:
            for chip in selected_chips:
                if 'synth' in chip.lower() or 'electronic' in chip.lower():
                    has_synth_preference = True
                    break
        
        if has_synth_preference and len(valid_indices) > 0:
            tracks_subset = self.df.iloc[valid_indices]
            acousticness_values = tracks_subset['acousticness'].values
            
            # Penalize high-acousticness tracks (40% penalty for acousticness > 0.5)
            acoustic_penalty = np.where(acousticness_values > 0.5, 
                                       (acousticness_values - 0.5) * 0.4,
                                       0)
            
            similarities = similarities - acoustic_penalty
            
            # Boost synth-pop genres (+20% for exact genre match)
            synth_pop_genres = ['synth-pop', 'electropop', 'synth', 'new-wave', 
                               'chillwave', 'dream-pop', 'indie-electronic']
            synth_boost = np.zeros(len(valid_indices))
            for idx, (df_idx, row) in enumerate(tracks_subset.iterrows()):
                genre_lower = str(row['track_genre']).lower()
                if any(sp_genre in genre_lower for sp_genre in synth_pop_genres):
                    synth_boost[idx] = 0.20  # 20% boost for synth-pop genres
            
            similarities = similarities + synth_boost
            similarities = np.clip(similarities, 0, 1)
            
            penalized_count = (acoustic_penalty > 0).sum()
            boosted_count = (synth_boost > 0).sum()
            print(f"ðŸŽ¸ Synth preference: penalized {penalized_count} guitar-heavy, boosted {boosted_count} synth-pop tracks")
        
        # Hyperpop specialization: Multi-tier genre and artist boosting
        # Applied when seed genre is hyperpop or related experimental genres
        if seed_genre and self.get_genre_category(seed_genre) == 'hyperpop' and len(valid_indices) > 0:
            tracks_subset = self.df.iloc[valid_indices]
            
            # Define tiered genre preferences for hyperpop listeners
            pure_electronic_genres = ['edm', 'house', 'techno', 'dubstep', 'drum-and-bass',
                                     'trance', 'electro', 'electronic', 'hardstyle', 'garage']
            electronic_pop_genres = ['synth-pop', 'electropop', 'dance-pop', 'dance', 'disco', 'club']
            pop_genres = ['pop', 'indie-pop', 'alt-pop']
            
            # Apply tiered genre boosting (20% â†’ 15% â†’ 5%)
            genre_boost = np.zeros(len(valid_indices))
            for idx, (df_idx, row) in enumerate(tracks_subset.iterrows()):
                genre_lower = str(row['track_genre']).lower()
                if any(pe_genre in genre_lower for pe_genre in pure_electronic_genres):
                    genre_boost[idx] = 0.20  # 20% boost for pure electronic
                elif any(ep_genre in genre_lower for ep_genre in electronic_pop_genres):
                    genre_boost[idx] = 0.15  # 15% boost for electronic pop
                elif any(p_genre in genre_lower for p_genre in pop_genres):
                    genre_boost[idx] = 0.05  # 5% boost for pop
            
            # Boost low-acousticness tracks (hyperpop is heavily produced)
            acousticness_values = tracks_subset['acousticness'].values
            acousticness_boost = np.where(acousticness_values < 0.3, 0.12, 0.0)  # 12% boost
            
            # Hyperpop artist whitelist boost (25% = highest boost in system)
            artist_boost = np.zeros(len(valid_indices))
            for idx, (df_idx, row) in enumerate(tracks_subset.iterrows()):
                artist_lower = str(row['artists']).lower()
                if any(hp_artist in artist_lower for hp_artist in self.hyperpop_artists):
                    artist_boost[idx] = 0.25  # 25% boost for whitelisted hyperpop artists
            
            similarities = similarities + genre_boost + acousticness_boost + artist_boost
            similarities = np.clip(similarities, 0, 1)
        
        # Popularity bias adjustment (if not 'balanced')
        # Modifies final similarity scores based on track popularity
        if popularity_bias != 'balanced' and len(valid_indices) > 0:
            popularity = self.df.iloc[valid_indices]['popularity'].values
            if popularity_bias == 'obscure':
                # Favor lower popularity (indie/underground bias)
                pop_weight = 1 - (popularity / 100)
            else:  # 'popular'
                # Favor higher popularity (mainstream bias)
                pop_weight = popularity / 100
            
            # Blend popularity weight with similarity using diversity_weight
            similarities = (similarities * (1 - diversity_weight) + 
                          pop_weight * diversity_weight)
        
        # Get top N*3 candidates (will be diversity-filtered to N)
        top_n = min(n * 3, len(similarities))
        if top_n == 0:
            return pd.DataFrame()
            
        top_indices = similarities.argsort()[-top_n:][::-1]  # Sort descending
        result_indices = valid_indices[top_indices]
        
        # Apply diversity enforcement (spatial distribution in result space)
        final_indices = self._apply_diversity(result_indices, n)
        
        # Build final recommendations DataFrame
        recommendations = self.df.iloc[final_indices].copy()
        recommendations['similarity_score'] = similarities[
            np.isin(valid_indices, final_indices)
        ]
        
        return recommendations[['track_name', 'artists', 'album_name', 
                               'track_genre', 'tempo', 'energy', 'valence', 
                               'danceability', 'popularity', 'similarity_score']]
    
    def _apply_language_filter(self, english_weight, total_tracks):
        """
        Tiered language filtering with strict English-only mode.
        
        Args:
            english_weight: 0.0 (international) to 1.0 (English only)
            total_tracks: Total number of tracks in dataset
            
        Returns:
            numpy.ndarray: Boolean mask indicating which tracks pass the filter
            
        Thresholds (empirically tuned):
        - >= 0.95: STRICT English only (100% Tier 1, exclude all others)
        - 0.80-0.95: Mostly English (~98% Tier 1, rare exceptions)
        - 0.40-0.80: Balanced mix (50% T1, 25% T2, 25% T3)
        - < 0.40: Prioritize international (30% T1, keep most T2/T3)
        
        Note: Tier 1=English, Tier 2=Western languages, Tier 3=International
        """
        mask = np.ones(total_tracks, dtype=bool)
        
        # Pre-compute tier masks for efficiency
        tier1_mask = (self.df['language_tier'] == 1).values  # English
        tier2_mask = (self.df['language_tier'] == 2).values  # Western
        tier3_mask = (self.df['language_tier'] == 3).values  # International
        
        # STRICT English-only mode (english_weight >= 0.95)
        if english_weight >= 0.95:
            # English ONLY - completely exclude all non-English tracks
            mask = tier1_mask
            print(f"  â†’ STRICT English only mode (100% English tracks)")
            print(f"  â†’ Excluded {(tier2_mask | tier3_mask).sum():,} non-English tracks")
        
        # Mostly English mode (english_weight 0.80-0.95)
        elif english_weight >= 0.8:
            # 80-95%: English ONLY with rare exceptions (keep ~2% non-English)
            tier2_positions = np.where(tier2_mask)[0]
            tier3_positions = np.where(tier3_mask)[0]
            
            # Remove almost ALL non-English tracks (98% exclusion rate)
            if len(tier2_positions) > 0:
                exclude_count = int(len(tier2_positions) * 0.98)
                if exclude_count > 0:
                    exclude = np.random.choice(tier2_positions, size=exclude_count, replace=False)
                    mask[exclude] = False
            
            if len(tier3_positions) > 0:
                exclude_count = int(len(tier3_positions) * 0.98)
                if exclude_count > 0:
                    exclude = np.random.choice(tier3_positions, size=exclude_count, replace=False)
                    mask[exclude] = False
            
            print(f"  â†’ Mostly English (~98% English tracks)")
        
        # Balanced mode (english_weight 0.40-0.80)
        elif english_weight >= 0.4:
            # 40-80%: Balanced mix (50% English, 25% Western, 25% International)
            tier1_positions = np.where(tier1_mask)[0]
            tier2_positions = np.where(tier2_mask)[0]
            tier3_positions = np.where(tier3_mask)[0]
            
            total_valid = len(tier1_positions) + len(tier2_positions) + len(tier3_positions)
            
            if total_valid > 0:
                # Target distribution: 50/25/25
                target_tier1 = int(total_valid * 0.5)
                target_tier2 = int(total_valid * 0.25)
                target_tier3 = int(total_valid * 0.25)
                
                # Randomly exclude excess tracks from each tier
                if len(tier1_positions) > target_tier1:
                    excess = len(tier1_positions) - target_tier1
                    exclude = np.random.choice(tier1_positions, size=excess, replace=False)
                    mask[exclude] = False
                
                if len(tier2_positions) > target_tier2:
                    excess = len(tier2_positions) - target_tier2
                    exclude = np.random.choice(tier2_positions, size=excess, replace=False)
                    mask[exclude] = False
                
                if len(tier3_positions) > target_tier3:
                    excess = len(tier3_positions) - target_tier3
                    exclude = np.random.choice(tier3_positions, size=excess, replace=False)
                    mask[exclude] = False
            
            print(f"  â†’ Balanced mix (English + Western + International)")
        
        # International priority mode (english_weight < 0.40)
        else:
            # <40%: Prioritize international (remove 70% of English tracks)
            tier1_positions = np.where(tier1_mask)[0]
            
            if len(tier1_positions) > 0:
                exclude_count = int(len(tier1_positions) * 0.7)
                if exclude_count > 0:
                    exclude = np.random.choice(tier1_positions, size=exclude_count, replace=False)
                    mask[exclude] = False
            
            print(f"  â†’ Prioritize international (Western + K-pop/J-pop/etc.)")
        
        return mask
    
    def _apply_diversity(self, indices, n):
        """
        Ensure recommendations are diverse through spatial distribution.
        
        Args:
            indices: Array of candidate track indices (pre-sorted by similarity)
            n: Number of final recommendations needed
            
        Returns:
            numpy.ndarray: Subset of indices with enforced diversity
            
        Strategy: Take every Kth track from sorted candidates to ensure
                 distribution across similarity space (prevents clustering)
        
        Example: If indices has 30 tracks and n=10, take every 3rd track
                [0, 3, 6, 9, 12, 15, 18, 21, 24, 27]
        """
        if len(indices) <= n:
            return indices
        
        # Calculate step size for even distribution
        step = len(indices) // n
        diverse_indices = indices[::max(1, step)][:n]
        
        return diverse_indices


# Testing and demonstration code
if __name__ == "__main__":
    """
    Example usage and testing for the HybridMusicRecommender.
    
    Demonstrates:
    - Basic initialization
    - Creating a test user profile
    - Generating recommendations with various filters
    """
    # Initialize with sample dataset for faster testing
    recommender = HybridMusicRecommender(use_sample=True)
    
    # Create a test profile: [tempo, energy, valence, danceability, acousticness,
    #                         instrumentalness, speechiness, liveness, loudness]
    test_profile = np.array([120, 0.6, 0.5, 0.6, 0.5, 0.3, 0.1, 0.2, -8])
    
    print("\nTesting with language preference:\n")
    
    # Generate recommendations with balanced language preference
    results = recommender.recommend(
        test_profile,
        n=10,
        bpm_range=(100, 140),
        popularity_range=(10, 80),
        similarity_method='hybrid',
        seed_genre='pop',
        user_location='US',
        english_weight=0.7  # Prefer English but allow some international tracks
    )
    
    if len(results) > 0:
        print(results[['track_name', 'artists', 'track_genre', 'similarity_score']].to_string(index=False))
    else:
        print("No recommendations found with current filters.")

    # References: 
    # Anthropic (2024) Claude 3.5 Sonnet. [Large language model]. Available at: https://claude.ai (Accessed: 18 November 2024).
